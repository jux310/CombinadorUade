{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GCK26i0LL0JkxrGjr5Kp395dlErF4Jui",
      "authorship_tag": "ABX9TyPiRi2XCqfFxbRnM+XG+Dm9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jux310/CombinadorUade/blob/main/Uade_Scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funcionamiento"
      ],
      "metadata": {
        "id": "DSjcb5RxgzzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Este programa simplifica la extracción y filtrado de materias, permitiendo una integración directa con herramientas de planificación de horarios.\n",
        "\n",
        "\n",
        "#### URL Principal:\n",
        "Es la dirección de la página web desde donde se extraen los datos de las materias.\n",
        "Ejemplo: La página institucional de UADE.\n",
        "\n",
        "```\n",
        "url = \"https://www.webcampus.uade.edu.ar/MiProgramacion...\n",
        "```\n",
        "\n",
        "#### HTM Filtro:\n",
        "Este filtro es para quitar las materias que estan bloqueadas por correlativas, las que no estan disponibles en el proximo periodo y las que no tienen cupos. Este archivo se obtiene entando a inscripciones.uade.edu.ar -> asignaturas cuatrimestre -> Seleccione sus materias y haciendo click derecho/guardar como sobre la tabla de materias. Se obtiene un archivo \"InscripcionClaseBuscar.htm\" el cual se sube al programa.\n",
        "\n",
        "```\n",
        "<Subir archivo \"InscripcionClaseBuscar.htm\">\n",
        "```\n",
        "\n",
        "#### Salida:\n",
        "El programa genera un archivo CSV llamado materias_completas el cual se descarga automaticamente.\n",
        "\n",
        "```\n",
        "Descargando materias_completas.csv\n",
        "```\n",
        "\n",
        "El CSV resultante se puede cargar en la sección de preferencias en https://uade.netlify.app para generar combinaciones de horarios a partir de las materias obtenidas.\n",
        "\n",
        "\n",
        "\n",
        "-\n",
        "\n"
      ],
      "metadata": {
        "id": "HZ9EqulFtHlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descargar Tutorial PDF"
      ],
      "metadata": {
        "id": "IZ1AysEpgloo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('UADE Scrapper Tutorial.pdf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "n7i7CAAUgVi5",
        "outputId": "54562ed6-6239-4e26-fad9-27e3733cea79"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_06cff76b-e3e4-46ef-9282-e496916bdb89\", \"UADE Scrapper Tutorial.pdf\", 3949583)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UADE Scrapper"
      ],
      "metadata": {
        "id": "KHlnTR6rgqGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import difflib  # Para la comparación aproximada\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Obtenga los datos necesarios desde https://inscripciones.uade.edu.ar/\\n\")\n",
        "url = input('Ingrese URL de Disponibilidad de clase:\\n')\n",
        "print(\"\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "html_filename = next(iter(uploaded))\n",
        "\n",
        "if html_content_uploaded:\n",
        "    soup = BeautifulSoup(html_content_uploaded, 'html.parser')\n",
        "    materia_tables = soup.find_all('table', class_='gridViewMaterias')\n",
        "    years = [span.text for span in soup.find_all('span', class_='anio')]\n",
        "    all_materia_data = []\n",
        "\n",
        "    for year_index, table in enumerate(materia_tables):\n",
        "        headers = [th.text.replace(\"<br>\", \" \").strip() for th in table.find('thead').find_all('th')] if table.find('thead') else [th.text.replace(\"<br>\", \" \").strip() for th in table.find('tr').find_all('th')]\n",
        "        rows = table.find('tbody').find_all('tr') if table.find('tbody') else table.find_all('tr')[1:]\n",
        "\n",
        "        year_name = years[year_index] if year_index < len(years) else \"Año Desconocido\"\n",
        "\n",
        "        for row in rows[1:]:\n",
        "            cols = row.find_all('td')\n",
        "            if not cols:\n",
        "                continue\n",
        "\n",
        "            materia_data = {}\n",
        "            for i, col in enumerate(cols):\n",
        "                header_text = headers[i] if i < len(headers) else f\"Columna{i+1}\"\n",
        "                cell_text = col.text.strip()\n",
        "                materia_data[header_text] = cell_text\n",
        "\n",
        "            materia_data['Año'] = year_name\n",
        "            all_materia_data.append(materia_data)\n",
        "\n",
        "    df = pd.DataFrame(all_materia_data)\n",
        "\n",
        "    df_filtrado = df[df['Información'] == '']\n",
        "    lista_nombres_materias = df_filtrado['Materia'].tolist()\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo leer el archivo HTML. Revise si subió el archivo correctamente y si no hubo errores al leerlo.\")\n",
        "\n",
        "http_headers = {\n",
        "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                    \"Chrome/91.0.4472.124 Safari/537.36\"),\n",
        "    \"Referer\": url\n",
        "}\n",
        "\n",
        "def eliminar_tildes(texto):\n",
        "    return ''.join(\n",
        "        char for char in unicodedata.normalize('NFKD', str(texto))\n",
        "        if not unicodedata.combining(char)\n",
        "    )\n",
        "\n",
        "def get_form_fields(soup):\n",
        "    \"\"\"\n",
        "    Recolecta todos los campos del formulario, incluyendo inputs hidden y selects.\n",
        "    \"\"\"\n",
        "    fields = {}\n",
        "    # Inputs hidden\n",
        "    for input_tag in soup.find_all('input', type='hidden'):\n",
        "        name = input_tag.get('name')\n",
        "        if name:\n",
        "            fields[name] = input_tag.get('value', '')\n",
        "    # Selects (dropdowns)\n",
        "    for select_tag in soup.find_all('select'):\n",
        "        name = select_tag.get('name')\n",
        "        if name:\n",
        "            selected_option = select_tag.find('option', selected=True)\n",
        "            if selected_option:\n",
        "                fields[name] = selected_option.get('value', '')\n",
        "            else:\n",
        "                fields[name] = ''\n",
        "    return fields\n",
        "\n",
        "def parse_table(soup):\n",
        "    \"\"\"\n",
        "    Extrae los datos de la tabla.\n",
        "    Primero intenta buscar la tabla por el id esperado, y si no la encuentra,\n",
        "    busca directamente en el <tbody>.\n",
        "    Para las columnas Col9 a Col15 (índices 8 a 14) se asigna True si el estilo contiene\n",
        "    \"background-color:LightGrey\", o False en caso contrario.\n",
        "    Solo se procesan filas que tengan 18 columnas.\n",
        "    \"\"\"\n",
        "    # Intentar encontrar la tabla por id\n",
        "    table = soup.find('table', {'id': 'ctl00_ContentPlaceHolderMain_RadGrid1_ctl00'})\n",
        "    if table:\n",
        "        tbody = table.find('tbody')\n",
        "    else:\n",
        "        tbody = soup.find('tbody')\n",
        "\n",
        "    if not tbody:\n",
        "        print(\"No se encontró el <tbody> en el HTML.\")\n",
        "        return [], []\n",
        "\n",
        "    rows = []\n",
        "    for row in tbody.find_all('tr'):\n",
        "        tds = row.find_all('td')\n",
        "        if not tds:\n",
        "            continue\n",
        "        row_data = []\n",
        "        # Procesar cada celda\n",
        "        for i, td in enumerate(tds):\n",
        "            # Para las columnas Col9 a Col15, evaluar el estilo\n",
        "            if i in range(8, 15):  # índices 8 a 14 (Col9 a Col15)\n",
        "                style = td.get(\"style\", \"\")\n",
        "                if \"background-color:LightGrey\" in style:\n",
        "                    row_data.append(True)\n",
        "                else:\n",
        "                    row_data.append(False)\n",
        "            else:\n",
        "                row_data.append(td.get_text(strip=True))\n",
        "        if len(row_data) != 18:\n",
        "            print(\"Fila con cantidad inesperada de columnas (se esperan 18):\", row_data)\n",
        "            continue  # Se omite la fila si no cumple la cantidad esperada\n",
        "        rows.append(row_data)\n",
        "\n",
        "    # Definir manualmente los encabezados (ajusta los nombres según corresponda)\n",
        "    headers = [\n",
        "        \"Código\", \"Materia\", \"Sede\", \"ID\", \"Modalidad\",\n",
        "        \"Idioma\", \"Turno\", \"Año\",\n",
        "        \"LUNES\", \"MARTES\", \"MIERCOLES\", \"JUEVES\", \"VIERNES\", \"SABADO\", \"DOMINGO\",\n",
        "        \"Horario\", \"Fechas\", \"Tipo\"\n",
        "    ]\n",
        "\n",
        "    return headers, rows\n",
        "\n",
        "def filter_materias(df, lista_nombres_materias, threshold=0.8): # Modificado para usar lista_nombres_materias\n",
        "    \"\"\"\n",
        "    Filtra el DataFrame 'df' dejando únicamente las materias que sean similares a\n",
        "    alguna de las materias listadas en  'lista_nombres_materias'.\n",
        "\n",
        "    Tanto los nombres en lista_nombres_materias como los del DataFrame se normalizan\n",
        "    (quitando espacios y pasando a minúsculas). Se utiliza difflib para comparar\n",
        "    de forma aproximada, aceptándose aquellas coincidencias cuyo ratio sea mayor\n",
        "    o igual a 'threshold'.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\nFiltrando materias: se usarán {len(lista_nombres_materias)} materias de la lista proporcionada.\")\n",
        "\n",
        "    # Normalizar la columna \"Materia\" del DataFrame\n",
        "    df['Materia'] = df['Materia'].str.replace('\"', '').str.replace('\"', '').str.replace(',', '')\n",
        "    df[\"Materia_norm\"] = df[\"Materia\"].astype(str).str.strip().str.lower().apply(eliminar_tildes)\n",
        "\n",
        "    # Preprocesar lista_nombres_materias: normalizar y pasar a minúsculas\n",
        "    lista_materias_norm = [eliminar_tildes(materia.strip().lower()) for materia in lista_nombres_materias]\n",
        "\n",
        "\n",
        "    # Función que verifica si la materia es similar a alguna materia en la lista\n",
        "    def is_similar(materia):\n",
        "        for m in lista_materias_norm:\n",
        "            ratio = difflib.SequenceMatcher(None, materia, m).ratio()\n",
        "            if ratio >= threshold:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # Aplicar la función a cada fila para determinar si se incluye\n",
        "    df_filtrado = df[df[\"Materia_norm\"].apply(is_similar)].copy()\n",
        "    df_filtrado.drop(columns=\"Materia_norm\", inplace=True)\n",
        "    return df_filtrado\n",
        "\n",
        "def filter_sede_pinamar(df):\n",
        "    \"\"\"\n",
        "    Filtra el DataFrame 'df' eliminando las filas donde la columna 'Sede'\n",
        "    contiene la palabra 'PINAMAR' (insensible a mayúsculas/minúsculas).\n",
        "    \"\"\"\n",
        "    print(\"\\nFiltrando sede: eliminando materias de PINAMAR.\")\n",
        "    df_filtrado = df[~df['Sede'].str.contains('PINAMAR', case=False)].copy()\n",
        "    return df_filtrado\n",
        "\n",
        "\n",
        "# Iniciar sesión\n",
        "session = requests.Session()\n",
        "\n",
        "# Primera carga: obtener parámetros iniciales y HTML completo\n",
        "response = session.get(url, headers=http_headers)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# (Opcional) Guardar la página inicial para revisar su estructura\n",
        "with open('pagina_inicial.html', 'w', encoding='utf-8') as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "# Obtener los campos del formulario\n",
        "form_data = get_form_fields(soup)\n",
        "\n",
        "# Primera solicitud POST con los parámetros obtenidos\n",
        "response = session.post(url, headers=http_headers, data=form_data)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Procesar la primera \"página\" de datos\n",
        "headers, data = parse_table(soup)\n",
        "all_data = data.copy()\n",
        "\n",
        "# Detectar paginación (si existe)\n",
        "pagination = soup.find('div', class_='rgInfoPart')\n",
        "total_pages = 1\n",
        "if pagination:\n",
        "    match = re.search(r'en (\\d+) paginas?\\(s\\)', pagination.get_text(strip=True))\n",
        "    if match:\n",
        "        total_pages = int(match.group(1))\n",
        "\n",
        "print(f\"\\nTotal de páginas: {total_pages}\")\n",
        "\n",
        "# Recorrer las páginas restantes (si total_pages > 1)\n",
        "for page in range(2, total_pages + 1):\n",
        "    # Actualizar los campos del formulario desde la página actual\n",
        "    form_data = get_form_fields(soup)\n",
        "\n",
        "    # Actualizar parámetros para paginación (verificar que __EVENTTARGET sea el correcto)\n",
        "    form_data.update({\n",
        "        '__EVENTTARGET': 'ctl00$ContentPlaceHolderMain$RadGrid1$ctl00$ctl03$ctl01$ctl'+str(6+2*total_pages),\n",
        "        '__EVENTARGUMENT': '',\n",
        "        '__LASTFOCUS': ''\n",
        "    })\n",
        "\n",
        "    # Solicitud POST para la siguiente página\n",
        "    response = session.post(url, headers=http_headers, data=form_data)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extraer datos de la página\n",
        "    _, page_data = parse_table(soup)\n",
        "    if page_data:\n",
        "        all_data.extend(page_data)\n",
        "    else:\n",
        "        print(f\"No se encontraron filas válidas en la página {page}.\")\n",
        "\n",
        "    print(f\"Procesada página {page}/{total_pages}\")\n",
        "\n",
        "# Crear DataFrame con los datos extraídos\n",
        "if headers and all_data:\n",
        "    try:\n",
        "        df = pd.DataFrame(all_data, columns=headers)\n",
        "        # Aplicar filtro usando lista_nombres_materias\n",
        "        df = filter_materias(df, lista_nombres_materias, threshold=0.8) # Modificado para usar lista_nombres_materias\n",
        "        # Aplicar filtro para eliminar la sede PINAMAR\n",
        "        df = filter_sede_pinamar(df) # Nueva línea para filtrar por sede PINAMAR\n",
        "        df.to_csv('materias_completas.csv', index=False, encoding='utf-8-sig')\n",
        "        print(\"Archivo guardado correctamente: materias_completas.csv\")\n",
        "        files.download('materias_completas.csv')\n",
        "        print(\"\\nImportar CSV a https://uade.netlify.app/\")\n",
        "    except Exception as e:\n",
        "        print(\"Error al crear el DataFrame:\", e)\n",
        "else:\n",
        "    print(\"No se encontraron datos válidos\")\n",
        "\n",
        "# Eliminar archivos intermedios\n",
        "!rm *.htm\n",
        "!rm *.html"
      ],
      "metadata": {
        "id": "6qlmru2vJM5z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}