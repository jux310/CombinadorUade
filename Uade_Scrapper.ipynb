{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxb/hqwZQaTn1DdY3/3M6a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jux310/CombinadorUade/blob/main/Uade_Scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cGXLgmB033d",
        "outputId": "3f304832-e440-4845-92f0-e0930a89c5b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de páginas: 3\n",
            "Procesada página 2/3\n",
            "Procesada página 3/3\n",
            "Filtrando materias: se encontraron 9 materias en la lista de filtro.\n",
            "Archivo guardado correctamente: materias_completas.csv\n",
            "Importar CSV a https://uade.netlify.app/\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import difflib  # Para la comparación aproximada\n",
        "\n",
        "# URL principal de la página a extraer datos\n",
        "url = \"https://www.webcampus.uade.edu.ar/MiProgramacion/Contenidos/MiProgramacion.aspx?IdUsuario=br+B3VmrZbw=\"\n",
        "\n",
        "# URL de filtro (si se deja vacío, no se aplica filtro)\n",
        "url_filtro = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vQ5Rw3QfNhw-TghB1XV3FMLjPsN9EKgr4e2Z0Awz169kXXB940iORYEwWE8RhiNeDmfgSFFyHPGZrh6/pub?gid=813457300&single=true&output=csv\"\n",
        "# url_filtro = \"\"  # Descomentar esta línea para no aplicar filtro\n",
        "\n",
        "http_headers = {\n",
        "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                   \"Chrome/91.0.4472.124 Safari/537.36\"),\n",
        "    \"Referer\": url\n",
        "}\n",
        "\n",
        "def eliminar_tildes(texto):\n",
        "    return ''.join(\n",
        "        char for char in unicodedata.normalize('NFKD', str(texto))\n",
        "        if not unicodedata.combining(char)\n",
        "    )\n",
        "\n",
        "def get_form_fields(soup):\n",
        "    \"\"\"\n",
        "    Recolecta todos los campos del formulario, incluyendo inputs hidden y selects.\n",
        "    \"\"\"\n",
        "    fields = {}\n",
        "    # Inputs hidden\n",
        "    for input_tag in soup.find_all('input', type='hidden'):\n",
        "        name = input_tag.get('name')\n",
        "        if name:\n",
        "            fields[name] = input_tag.get('value', '')\n",
        "    # Selects (dropdowns)\n",
        "    for select_tag in soup.find_all('select'):\n",
        "        name = select_tag.get('name')\n",
        "        if name:\n",
        "            selected_option = select_tag.find('option', selected=True)\n",
        "            if selected_option:\n",
        "                fields[name] = selected_option.get('value', '')\n",
        "            else:\n",
        "                fields[name] = ''\n",
        "    return fields\n",
        "\n",
        "def parse_table(soup):\n",
        "    \"\"\"\n",
        "    Extrae los datos de la tabla.\n",
        "    Primero intenta buscar la tabla por el id esperado, y si no la encuentra,\n",
        "    busca directamente en el <tbody>.\n",
        "    Para las columnas Col9 a Col15 (índices 8 a 14) se asigna True si el estilo contiene\n",
        "    \"background-color:LightGrey\", o False en caso contrario.\n",
        "    Solo se procesan filas que tengan 18 columnas.\n",
        "    \"\"\"\n",
        "    # Intentar encontrar la tabla por id\n",
        "    table = soup.find('table', {'id': 'ctl00_ContentPlaceHolderMain_RadGrid1_ctl00'})\n",
        "    if table:\n",
        "        tbody = table.find('tbody')\n",
        "    else:\n",
        "        tbody = soup.find('tbody')\n",
        "\n",
        "    if not tbody:\n",
        "        print(\"No se encontró el <tbody> en el HTML.\")\n",
        "        return [], []\n",
        "\n",
        "    rows = []\n",
        "    for row in tbody.find_all('tr'):\n",
        "        tds = row.find_all('td')\n",
        "        if not tds:\n",
        "            continue\n",
        "        row_data = []\n",
        "        # Procesar cada celda\n",
        "        for i, td in enumerate(tds):\n",
        "            # Para las columnas Col9 a Col15, evaluar el estilo\n",
        "            if i in range(8, 15):  # índices 8 a 14 (Col9 a Col15)\n",
        "                style = td.get(\"style\", \"\")\n",
        "                if \"background-color:LightGrey\" in style:\n",
        "                    row_data.append(True)\n",
        "                else:\n",
        "                    row_data.append(False)\n",
        "            else:\n",
        "                row_data.append(td.get_text(strip=True))\n",
        "        if len(row_data) != 18:\n",
        "            print(\"Fila con cantidad inesperada de columnas (se esperan 18):\", row_data)\n",
        "            continue  # Se omite la fila si no cumple la cantidad esperada\n",
        "        rows.append(row_data)\n",
        "\n",
        "    # Definir manualmente los encabezados (ajusta los nombres según corresponda)\n",
        "    headers = [\n",
        "        \"Código\", \"Materia\", \"Sede\", \"ID\", \"Modalidad\",\n",
        "        \"Idioma\", \"Turno\", \"Año\",\n",
        "        \"LUNES\", \"MARTES\", \"MIERCOLES\", \"JUEVES\", \"VIERNES\", \"SABADO\", \"DOMINGO\",\n",
        "        \"Horario\", \"Fechas\", \"Tipo\"\n",
        "    ]\n",
        "\n",
        "    return headers, rows\n",
        "\n",
        "def filter_materias(df, url_filtro, threshold=0.4):\n",
        "    \"\"\"\n",
        "    Filtra el DataFrame 'df' dejando únicamente las materias que sean similares a\n",
        "    alguna de las materias listadas en la URL de filtro (CSV).\n",
        "\n",
        "    Se ignora la columna B del CSV y se utiliza únicamente la columna A. Tanto\n",
        "    los nombres en el CSV como los del DataFrame se normalizan (quitando espacios y\n",
        "    pasando a minúsculas). Se utiliza difflib para comparar de forma aproximada,\n",
        "    aceptándose aquellas coincidencias cuyo ratio sea mayor o igual a 'threshold'.\n",
        "\n",
        "    Si url_filtro es una cadena vacía, no se aplica ningún filtro.\n",
        "    \"\"\"\n",
        "    if not url_filtro.strip():\n",
        "        print(\"No se aplica filtro (url_filtro vacía).\")\n",
        "        return df\n",
        "\n",
        "    try:\n",
        "        # Leer el CSV desde la URL\n",
        "        filtro_df = pd.read_csv(url_filtro)\n",
        "        # Usar solo la primera columna (columna A) y limpiar espacios y pasar a minúsculas\n",
        "        lista_materias = filtro_df.iloc[:, 0].astype(str).str.strip().str.lower().tolist()\n",
        "        print(f\"Filtrando materias: se encontraron {len(lista_materias)} materias en la lista de filtro.\")\n",
        "\n",
        "        # Normalizar la columna \"Materia\" del DataFrame\n",
        "        df[\"Materia_norm\"] = df[\"Materia\"].astype(str).str.strip().str.lower().str.replace('\"', '').str.replace('\"', '').apply(eliminar_tildes)\n",
        "\n",
        "        # Función que verifica si la materia es similar a alguna materia en la lista\n",
        "        def is_similar(materia):\n",
        "            for m in lista_materias:\n",
        "                ratio = difflib.SequenceMatcher(None, materia, m).ratio()\n",
        "                if ratio >= threshold:\n",
        "                    return True\n",
        "            return False\n",
        "\n",
        "        # Aplicar la función a cada fila para determinar si se incluye\n",
        "        df_filtrado = df[df[\"Materia_norm\"].apply(is_similar)].copy()\n",
        "        df_filtrado.drop(columns=\"Materia_norm\", inplace=True)\n",
        "        return df_filtrado\n",
        "    except Exception as e:\n",
        "        print(\"Error al filtrar materias:\", e)\n",
        "        return df\n",
        "\n",
        "# Iniciar sesión\n",
        "session = requests.Session()\n",
        "\n",
        "# Primera carga: obtener parámetros iniciales y HTML completo\n",
        "response = session.get(url, headers=http_headers)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# (Opcional) Guardar la página inicial para revisar su estructura\n",
        "with open('pagina_inicial.html', 'w', encoding='utf-8') as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "# Obtener los campos del formulario\n",
        "form_data = get_form_fields(soup)\n",
        "\n",
        "# Primera solicitud POST con los parámetros obtenidos\n",
        "response = session.post(url, headers=http_headers, data=form_data)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Procesar la primera \"página\" de datos\n",
        "headers, data = parse_table(soup)\n",
        "all_data = data.copy()\n",
        "\n",
        "# Detectar paginación (si existe)\n",
        "pagination = soup.find('div', class_='rgInfoPart')\n",
        "total_pages = 1\n",
        "if pagination:\n",
        "    match = re.search(r'en (\\d+) paginas?\\(s\\)', pagination.get_text(strip=True))\n",
        "    if match:\n",
        "        total_pages = int(match.group(1))\n",
        "\n",
        "print(f\"Total de páginas: {total_pages}\")\n",
        "\n",
        "# Recorrer las páginas restantes (si total_pages > 1)\n",
        "for page in range(2, total_pages + 1):\n",
        "    # Actualizar los campos del formulario desde la página actual\n",
        "    form_data = get_form_fields(soup)\n",
        "\n",
        "    # Actualizar parámetros para paginación (verificar que __EVENTTARGET sea el correcto)\n",
        "    form_data.update({\n",
        "        '__EVENTTARGET': 'ctl00$ContentPlaceHolderMain$RadGrid1$ctl00$ctl03$ctl01$ctl12',\n",
        "        '__EVENTARGUMENT': '',\n",
        "        '__LASTFOCUS': ''\n",
        "    })\n",
        "\n",
        "    # Solicitud POST para la siguiente página\n",
        "    response = session.post(url, headers=http_headers, data=form_data)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extraer datos de la página\n",
        "    _, page_data = parse_table(soup)\n",
        "    if page_data:\n",
        "        all_data.extend(page_data)\n",
        "    else:\n",
        "        print(f\"No se encontraron filas válidas en la página {page}.\")\n",
        "\n",
        "    print(f\"Procesada página {page}/{total_pages}\")\n",
        "\n",
        "# Crear DataFrame con los datos extraídos\n",
        "if headers and all_data:\n",
        "    try:\n",
        "        df = pd.DataFrame(all_data, columns=headers)\n",
        "        # Aplicar filtro si se ha especificado una url_filtro\n",
        "        df = filter_materias(df, url_filtro, threshold=0.8)\n",
        "        df.to_csv('materias_completas.csv', index=False, encoding='utf-8-sig')\n",
        "        print(\"Archivo guardado correctamente: materias_completas.csv\")\n",
        "        print(\"Importar CSV a https://uade.netlify.app/\")\n",
        "    except Exception as e:\n",
        "        print(\"Error al crear el DataFrame:\", e)\n",
        "else:\n",
        "    print(\"No se encontraron datos válidos\")\n"
      ]
    }
  ]
}